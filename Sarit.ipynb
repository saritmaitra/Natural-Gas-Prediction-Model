{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saritmaitra/Natural-Gas_Paul/blob/master/Sarit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQsr3P2xQQSD",
        "colab_type": "text"
      },
      "source": [
        "# **Data mining approach**\n",
        "## **Data ingestion, processing, feature engineering and machine learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtNdbYBZchuH",
        "colab_type": "text"
      },
      "source": [
        "#**Code without ngrok**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bbvw-yEaH_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyforest\n",
        "from pyforest import *\n",
        "import datetime, pickle, copy\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 150)\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "!pip install quandl\n",
        "import quandl\n",
        "!pip install mpl_finance\n",
        "!pip install python_wtd\n",
        "from python_wtd import WTD\n",
        "plt.style.use('ggplot')\n",
        "from statistics import variance \n",
        "from random import randint\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, accuracy_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "!pip install fredapi\n",
        "from fredapi import Fred\n",
        "!pip install EIA_python\n",
        "import eia\n",
        "!pip install python_wtd\n",
        "from python_wtd import WTD\n",
        "!pip install termcolor\n",
        "from termcolor import colored\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate, train_test_split, TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "#print(NG)\n",
        "\n",
        "\n",
        "# Cushing, OK WTI Spot Price FOB, Daily\n",
        "print('\\033[4mWTI Spot Price FOB, Daily (Dollars per Barrel)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    wti = pd.DataFrame(series_search)\n",
        "    return wti\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='PET.RWTC.D'\n",
        "      wti = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(wti))\n",
        "      return wti;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "wti = main()\n",
        "wti = wti.rename({'Cushing, OK WTI Spot Price FOB, Daily (Dollars per Barrel)': 'wti'}, axis = 'columns')\n",
        "wti = wti.reset_index()\n",
        "wti['index'] = pd.to_datetime(wti['index'].str[:-3], format='%Y %m%d')\n",
        "wti['Date']= pd.to_datetime(wti['index']) \n",
        "wti.set_index('Date', inplace=True) # setting index column\n",
        "wti = wti.loc['2000-01-01':,['wti']] # setting date range\n",
        "wti = wti.astype(float)\n",
        "#print(wti) \n",
        "#print('\\n')\n",
        "\n",
        "## GDP data\n",
        "print('\\033[4mUS GDP Data\\033[0m')\n",
        "fred = Fred(api_key='59c798fcda5850ef874412d269fd2378')\n",
        "gdp = fred.get_series_as_of_date('GDP', '2020-01-31')\n",
        "gdp = gdp.rename({'date': 'Date', 'value': 'gdp'}, axis = 'columns')\n",
        "gdp.set_index('Date', inplace=True) # setting index column\n",
        "gdp = gdp.loc['2000-01-01':'2020-02-02',['gdp']] # setting date range\n",
        "gdp.replace({pd.NaT: \"0\"}, inplace=True) # replacing NaT with 0\n",
        "gdp = gdp.ffill().bfill()\n",
        "gdp = gdp.reset_index().drop_duplicates('Date').set_index('Date') # dropping duplicate concurrence\n",
        "gdp = pd.DataFrame(gdp)\n",
        "gdp = gdp.astype(float)\n",
        "gdp = gdp.resample('B').ffill()\n",
        "gdp = gdp/4\n",
        "#print(gdp)\n",
        "#print('\\n')\n",
        "\n",
        "# U.S. Natural Gas Marketed Production, Monthly\n",
        "print('\\033[4mNatural Gas Marketed Production, Monthly (Million Cubic Feet)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    prod = pd.DataFrame(series_search)\n",
        "    return prod\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='NG.N9050US2.M'\n",
        "      prod = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(prod))\n",
        "      return prod;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "prod = main()\n",
        "prod = prod.rename({'U.S. Natural Gas Marketed Production, Monthly (Million Cubic Feet)': 'prod'}, axis = 'columns')\n",
        "prod = prod.reset_index()\n",
        "prod['Date']= pd.to_datetime(prod['index']) \n",
        "prod.set_index('Date', inplace=True) # setting index column\n",
        "prod = prod.loc['2000-01-01':,['prod']] # setting date range\n",
        "prod = prod.astype(float)\n",
        "prod = prod.resample('B').ffill()\n",
        "prod = prod/21\n",
        "#print(prod)\n",
        "#print('\\n')\n",
        "\n",
        "\n",
        "# U.S. Natural Gas Gross Withdrawals, Monthly\n",
        "print('\\033[4mNatural Gas Gross Withdrawals, Monthly (Million Cubic Feet)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    withdrawals = pd.DataFrame(series_search)\n",
        "    return withdrawals\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='NG.N9010US2.M' \n",
        "      withdrawals = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(withdrawals))\n",
        "      return withdrawals;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "withdrawals = main()\n",
        "withdrawals = withdrawals.rename({'U.S. Natural Gas Gross Withdrawals, Monthly (Million Cubic Feet)': 'withdrawals'}, axis = 'columns')\n",
        "withdrawals = withdrawals.reset_index()\n",
        "withdrawals['Date']= pd.to_datetime(withdrawals['index']) \n",
        "withdrawals.set_index('Date', inplace=True) # setting index column\n",
        "withdrawals = withdrawals.loc['2000-01-01':,['withdrawals']] # setting date range\n",
        "withdrawals = withdrawals.astype(float)\n",
        "withdrawals = withdrawals.resample('B').bfill().ffill()\n",
        "withdrawals = withdrawals/21\n",
        "#print(withdrawals)\n",
        "#print('\\n')\n",
        "\n",
        "# U.S. Natural Gas Underground Storage Volume, Monthly\n",
        "print('\\033[4mNatural Gas Underground Storage Volume, Monthly\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    storage = pd.DataFrame(series_search)\n",
        "    return storage\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='NG.N5030US2.M '\n",
        "      storage = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(storage))\n",
        "      return storage;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "storage = main()\n",
        "storage = storage.rename({'U.S. Natural Gas Underground Storage Volume, Monthly (Million Cubic Feet)': 'storage'}, axis = 'columns')\n",
        "storage = storage.reset_index()\n",
        "storage['Date']= pd.to_datetime(storage['index']) \n",
        "storage.set_index('Date', inplace=True) # setting index column\n",
        "storage = storage.loc['2000-01-01':,['storage']] # setting date range\n",
        "stoarge = storage.astype(float)\n",
        "storage = storage.resample('B').bfill().ffill()\n",
        "storage = storage/21\n",
        "#print(storage)\n",
        "#print('\\n')\n",
        "\n",
        "# U.S. Natural Gas Exports, Monthly\n",
        "print('\\033[4mNatural Gas Exports, Monthly (Million Cubic Feet)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    export = pd.DataFrame(series_search)\n",
        "    return export\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='NG.N9133US2.M'\n",
        "      export = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(export))\n",
        "      return export;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "export = main()\n",
        "export = export.rename({'Liquefied U.S. Natural Gas Exports, Monthly (Million Cubic Feet)': 'export'}, axis = 'columns')\n",
        "export = export.reset_index()\n",
        "export['Date']= pd.to_datetime(export['index']) \n",
        "export.set_index('Date', inplace=True) # setting index column\n",
        "export = export.loc['2000-01-01':,['export']] # setting date range\n",
        "export = export.astype(float)\n",
        "export = export.resample('B').bfill().ffill()\n",
        "export = export/21\n",
        "#print(export)\n",
        "#print('\\n')\n",
        "\n",
        "# natural gas total consumption (monthly))\n",
        "print('\\033[4mNatural Gas Total Consumption, Monthly (Million Cubic Feet)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    consumption = pd.DataFrame(series_search)\n",
        "    return consumption\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='NG.N9140US2.M'\n",
        "      consumption = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(consumption))\n",
        "      return consumption;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "consumption = main()\n",
        "consumption = consumption.rename({'U.S. Natural Gas Total Consumption, Monthly (Million Cubic Feet)': 'consumption'}, axis = 'columns')\n",
        "consumption = consumption.reset_index()\n",
        "consumption['Date']= pd.to_datetime(consumption['index']) \n",
        "consumption.set_index('Date', inplace=True) # setting index column\n",
        "consumption = consumption.loc['2000-01-01':,['consumption']] # setting date range\n",
        "consumption = consumption.astype(float)\n",
        "consumption = consumption.resample('B').ffill().bfill()\n",
        "consumption = consumption/21\n",
        "#print(consumption)\n",
        "#print('\\n')\n",
        "\n",
        "# New York Harbor No. 2 Heating Oil Spot Price FOB, Monthly\n",
        "print('\\033[4mHeating Oil Spot Price FOB, Daily (Dollars per Gallon)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    heatingoil = pd.DataFrame(series_search)\n",
        "    return heatingoil\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='PET.EER_EPD2F_PF4_Y35NY_DPG.D'\n",
        "      heatingoil = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(heatingoil))\n",
        "      return heatingoil;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "\n",
        "heatingoil = main()\n",
        "heatingoil = heatingoil.rename({'New York Harbor No. 2 Heating Oil Spot Price FOB, Daily (Dollars per Gallon)': 'heatingoil'}, axis = 'columns')\n",
        "heatingoil = heatingoil.reset_index()\n",
        "heatingoil['index'] = pd.to_datetime(heatingoil['index'].str[:-3], format='%Y %m%d')\n",
        "heatingoil['Date']= pd.to_datetime(heatingoil['index']) \n",
        "heatingoil.set_index('Date', inplace=True) # setting index column\n",
        "heatingoil = heatingoil.loc['2000-01-01':,['heatingoil']] # setting date range\n",
        "heatingoil = heatingoil.astype(float)\n",
        "#print(heatingoil)\n",
        "#print('\\n')\n",
        "\n",
        "# natural gas import dat\n",
        "#print('\\033[4mNatural Gas Imports, Monthly (Million Cubic Feet)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    g_import = pd.DataFrame(series_search)\n",
        "    return g_import\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='NG.N9100US2.M'\n",
        "      g_import = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(g_import))\n",
        "      return g_import;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "  \n",
        "g_import = main()\n",
        "g_import = g_import.rename({'U.S. Natural Gas Imports, Monthly (Million Cubic Feet)': 'g_import'}, axis = 'columns')\n",
        "g_import = g_import.reset_index()\n",
        "g_import['Date']= pd.to_datetime(g_import['index']) \n",
        "g_import.set_index('Date', inplace=True) # setting index column\n",
        "g_import = g_import.loc['2000-01-01':,['g_import']] # setting date range\n",
        "g_import = g_import.astype(float)\n",
        "g_import = g_import.resample('B').ffill().bfill()\n",
        "g_import = g_import/21\n",
        "#print(g_import)\n",
        "#print('\\n')\n",
        "\n",
        "# U.S. Natural Gas Rotary Rigs in Operation, Monthly\n",
        "print('\\033[4mNatural Gas Rotary Rigs in Operation, Monthly (Number of Elements)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    drilling = pd.DataFrame(series_search)\n",
        "    return drilling\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='PET.E_ERTRRG_XR0_NUS_C.M '\n",
        "      drilling = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(drilling))\n",
        "      return drilling;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "  \n",
        "drilling = main()\n",
        "drilling = drilling.rename({'U.S. Natural Gas Rotary Rigs in Operation, Monthly (Number of Elements)': 'drilling'}, axis = 'columns')\n",
        "drilling = drilling.reset_index()\n",
        "drilling['Date']= pd.to_datetime(drilling['index']) \n",
        "drilling.set_index('Date', inplace=True) # setting index column\n",
        "drilling = drilling.loc['2000-01-01':,['drilling']] # setting date range\n",
        "drilling = drilling.astype(float)\n",
        "drilling = drilling.resample('B').ffill().bfill()\n",
        "drilling = drilling/21\n",
        "#print(drilling)\n",
        "#print('\\n')\n",
        "\n",
        "# Heating Degree Days U.S. Average , Monthly\n",
        "print('\\033[4mHeating Degree Days U.S. Average , Monthly (total degree days)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    HDD_US = pd.DataFrame(series_search)\n",
        "    return HDD_US\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='STEO.ZWHDPUS.M'\n",
        "      HDD_US = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(HDD_US))\n",
        "      return HDD_US;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "  \n",
        "HDD_US = main()\n",
        "HDD_US = HDD_US.rename({'Heating Degree Days U.S. Average , Monthly (total degree days)': 'HDD_US'}, axis = 'columns')\n",
        "HDD_US = HDD_US.reset_index()\n",
        "HDD_US['Date']= pd.to_datetime(HDD_US['index']) \n",
        "HDD_US.set_index('Date', inplace=True) # setting index column\n",
        "HDD_US = HDD_US.loc['2000-01-01':,['HDD_US']] # setting date range\n",
        "HDD_US = HDD_US.astype(float)\n",
        "HDD_US = HDD_US.resample('B').ffill().bfill()\n",
        "HDD_US = HDD_US/21\n",
        "#print(HDD_US)\n",
        "#print('\\n')\n",
        "\n",
        "# Cooling Degree-Days, United States, Monthly\n",
        "print('\\033[4mCooling Degree-Days, United States, Monthly (Number)\\033[0m')\n",
        "def retrieve_time_series(api, series_ID):\n",
        "    \"\"\"\n",
        "    Return the time series dataframe, based on API and unique Series ID\n",
        "    \"\"\"\n",
        "    #Retrieve Data By Series ID \n",
        "    series_search = api.data_by_series(series=series_ID)\n",
        "    ##Create a pandas dataframe from the retrieved time series\n",
        "    CDD_US = pd.DataFrame(series_search)\n",
        "    return CDD_US\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run main script\n",
        "    \"\"\"\n",
        "    try:\n",
        "      #Create EIA API using your specific API key\n",
        "      api_key = \"ad819ee5a69e69390eadf300fa168fa8\"\n",
        "      api = eia.API(api_key)\n",
        "      #Declare desired series ID\n",
        "      series_ID='TOTAL.ZWCDPUS.M'\n",
        "      CDD_US = retrieve_time_series(api, series_ID)\n",
        "      #Print the returned dataframe df\n",
        "      print(type(CDD_US))\n",
        "      return CDD_US;\n",
        "    except Exception as e:\n",
        "      print(\"error\", e)\n",
        "      return pd.DataFrame(columns=None)\n",
        "  \n",
        "CDD_US = main()\n",
        "CDD_US = CDD_US.rename({'Cooling Degree-Days, United States, Monthly (Number)': 'CDD_US'}, axis = 'columns')\n",
        "CDD_US = CDD_US.reset_index()\n",
        "CDD_US['Date']= pd.to_datetime(CDD_US['index']) \n",
        "CDD_US.set_index('Date', inplace=True) # setting index column\n",
        "CDD_US = CDD_US.loc['2000-01-01':,['CDD_US']] # setting date range\n",
        "CDD_US = CDD_US.astype(float)\n",
        "CDD_US = CDD_US.resample('B').ffill().bfill()\n",
        "CDD_US = CDD_US/21\n",
        "#print(CDD_US)\n",
        "#print('\\n')\n",
        "\n",
        "# merging data frames\n",
        "merge1 = NG.join(gdp, how='left').ffill().bfill()\n",
        "merge2 = merge1.join(wti, how = 'left')\n",
        "merge3 = merge2.join(withdrawals, how = 'left').ffill().bfill()\n",
        "merge4 = merge3.join(storage, how = 'left').ffill().bfill()\n",
        "merge5 = merge4.join(prod, how = 'left').ffill().bfill()\n",
        "merge6 = merge5.join(consumption, how = 'left').ffill().bfill()\n",
        "merge7 = merge6.join(heatingoil, how ='left').ffill().bfill()\n",
        "merge8 = merge7.join(drilling, how = 'left').ffill().bfill()\n",
        "merge9 = merge8.join(export, how = 'left').ffill().bfill()\n",
        "merge10 = merge9.join(g_import, how = 'left').ffill().bfill()\n",
        "merge11 = merge10.join(HDD_US, how = 'left').ffill().bfill()\n",
        "merge12 = merge11.join(CDD_US, how = 'left').ffill().bfill()\n",
        "\n",
        "# feature engineering\n",
        "merge12['day_of_week'] = merge12.index.dayofweek\n",
        "merge12['day_of_month'] = merge12.index.day\n",
        "merge12['quarter'] = merge12.index.quarter\n",
        "merge12['month'] = merge12.index.month\n",
        "merge12['year'] = merge12.index.year\n",
        "\n",
        "merge13 = merge12.apply(copy.deepcopy)\n",
        "#print(merge13)\n",
        "\n",
        "# Technical indicators\n",
        "merge13['daily_ret'] = merge13['Last'].pct_change()\n",
        "merge13['volatility'] = merge13['daily_ret'].rolling(252).std()*(252**0.5)\n",
        "merge13[\"close_1\"] = merge13['Last'].shift(1)\n",
        "merge13[\"close_incr\"] = merge13['Last'] - merge13['Last'].shift(1)\n",
        "merge13['price_diff'] = merge13['Last'] - merge13['Open']\n",
        "merge13[\"vol_increment\"] = merge13['Volume'].diff()\n",
        "merge13[\"vol_rel_increment\"] = merge13['Volume'].diff() / merge13['Volume']\n",
        "\n",
        "sma1 = 20\n",
        "sma2 = 100\n",
        "merge13[\"sma1\"] = merge13['Last'].rolling(sma1).mean()\n",
        "merge13[\"sma2\"] = merge13['Last'].rolling(sma2).mean()\n",
        "#merge13['ema42'] = merge13['Open'].ewm(span=42).mean()\n",
        "#merge13['ema252'] = merge13['Open'].ewm(span=252).mean()\n",
        "\n",
        "merge13['ema_12'] = merge13['Last'].ewm(span=10).mean()\n",
        "merge13['ema_26'] = merge13['Last'].ewm(span=26).mean()\n",
        "merge13['ROC'] = ((merge13['Last'] - merge13['Last'].shift(5)) / (merge13['Last'].shift(5)))*100\n",
        "\n",
        "delta = merge13['Last'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge13['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge13['macd'] = merge13['ema_12'] - merge13['ema_26']\n",
        "\n",
        "#print('\\033[4mMerged dataframe\\033[0m')\n",
        "#print(df.info())\n",
        "#print('\\n')\n",
        "\n",
        "\n",
        "#print('\\033[4mProcessed dataframe\\033[0m')\n",
        "df = merge13.apply(copy.deepcopy)\n",
        "df.fillna(-99999, inplace=True)\n",
        "#print(df)\n",
        "#print(df.info())\n",
        "#print('\\n')\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "fig = go.Figure(data=[go.Candlestick(x=NG.index,\n",
        "                open=NG['Open'],\n",
        "                high=NG['High'],\n",
        "                low=NG['Low'],\n",
        "                close=NG['Last'])])\n",
        "fig.show()\n",
        "print('\\n')\n",
        "\n",
        "#fig = plt.figure(figsize=(20,6))\n",
        "#sns.lineplot(x='day_of_week', y= 'Open', data=df)\n",
        "#plt.title('Natural gas continuous contract 1- day of the week opening price from 2000 to till date')\n",
        "#plt.show()\n",
        "#print('\\n')\n",
        "\n",
        "#fig = plt.figure(figsize=(20,6))\n",
        "#sns.lineplot(x='month', y= 'Open', data=df);\n",
        "#plt.title('Natural gas continuous contract 1- Monthly opening price from 2000 to till date', fontsize = 12)\n",
        "#plt.show()\n",
        "#print('\\n')\n",
        "\n",
        "# Calculate the daily percentage change which is daily return\n",
        "#print('\\033[1m' + 'daily percentage change' + '\\033[1m')\n",
        "daily_ret = merge13['Last'].pct_change().fillna(0)\n",
        "mean_return = daily_ret.mean()\n",
        "return_stdev = daily_ret.std()\n",
        "print('Average daily return:',   mean_return )\n",
        "print('Average Std deviation (Volatility):',  return_stdev)\n",
        "\n",
        "print ('Annualized average return of Natural Gas (Contract#1):', round(mean_return * 252,2))\n",
        "print('Annualized volatility of Natural Gas (Contract#1):', round(return_stdev * np.sqrt(252), 2))\n",
        "print('\\n')\n",
        "\n",
        "#print('\\033[4mStatistics summary\\033[0m')\n",
        "#print(merge12.daily_ret.describe())\n",
        "#print('\\n')\n",
        "\n",
        "#print('\\033[4mCritical Values\\033[0m')\n",
        "n = len(daily_ret)\n",
        "test_statistic = ((daily_ret.mean() - 0) / (daily_ret.std()/np.sqrt(n)))\n",
        "#print ('t test statistic: ', test_statistic)\n",
        "#print('\\n')\n",
        "\n",
        "from scipy.stats import t\n",
        "p_val = 2 * (1 - t.cdf(test_statistic, n - 1))\n",
        "#print ('P-value is: ', p_val)\n",
        "#print('\\n')\n",
        "\n",
        "from scipy.stats import chi2\n",
        "# Here we calculate the critical value directly because our df is too high for most chisquare tables\n",
        "crit_value = chi2.ppf(0.99, (n - 1))\n",
        "#print ('Critical value at α = 0.01 with 251 df: ', crit_value)\n",
        "#print('\\n')\n",
        "\n",
        "# +/- %5 daily change distribution in Henry Hub gas prices\n",
        "#daily_ret.hist(bins=100, range = (-0.05, 0.05), figsize = (15,8))\n",
        "#plt.title('+/- 5% daily change distribution in Henry Hub gas prices')\n",
        "#plt.show()\n",
        "#print('\\n')\n",
        "\n",
        "# Plot the distributions\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
        "daily_ret.plot.hist(bins = 60)\n",
        "ax1.set_xlabel(\"Daily returns %\")\n",
        "ax1.set_ylabel(\"Percent\")\n",
        "ax1.set_title(\"Natural Gas daily returns data\")\n",
        "ax1.text(-0.25,75,\"Extreme Low\\nreturns\")\n",
        "ax1.text(0.15,75,\"Extreme High\\nreturns\")\n",
        "plt.show()\n",
        "print('\\n')\n",
        "#print ('\\033[1m' + 'Shapiro test: W-test, and P-value' + '\\033[1m')\n",
        "#print(stats.shapiro(daily_ret))\n",
        "#print('\\n')\n",
        "#print(\"Skewness, Kurtosis : \", daily_ret.skew(), daily_ret.kurtosis())\n",
        "#print('\\n')\n",
        "#print(\"T-value P-value (two-tail)\")\n",
        "#print(stats.ttest_1samp(daily_ret, 0.5))\n",
        "#print(stats.ttest_1samp(daily_ret, 0))\n",
        "#print('\\n')\n",
        "\n",
        "print('\\033[4mMaximum Drawdown\\033[0m')\n",
        "# Using a trailing 252 trading day window\n",
        "window = 252\n",
        "# Calculate the max drawdown in the past window days for each day in the series.\n",
        "# Using min_periods=1, we want to let the first 252 days data have an expanding window\n",
        "roll_max = df['Last'].rolling(min_periods=1, window=window).max()\n",
        "# Calculate daily draw-down from rolling max\n",
        "daily_drawdown = df['Last']/roll_max - 1.0\n",
        "print('\\033[4mMaximum Daily Drawdown\\033[0m')\n",
        "print(daily_drawdown)\n",
        "print('\\n')\n",
        "\n",
        "# Next we calculate the minimum (negative) daily drawdown in that window.\n",
        "# using min_periods=1 to allow the expanding window\n",
        "print('\\033[4mMin Daily Drawdown\\033[0m')\n",
        "min_daily_drawdown = daily_drawdown.rolling(min_periods=1, window=window).min()\n",
        "print(min_daily_drawdown)\n",
        "print('\\n')\n",
        "\n",
        "# Plot the results\n",
        "daily_drawdown.plot(figsize=(20,8))\n",
        "min_daily_drawdown.plot()\n",
        "plt.title('Orange -> Daily running 252-day drawdown; Blue -> Max. experienced 252-day drawdown in the past year')\n",
        "plt.suptitle('Maximum daily draw-down')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mProbability of +/-(1%); +/-(3%); +/-%(5) change in gas prices (Data -> 2000- till date)\\033[0m')\n",
        "#print('\\033[1m' + 'Probability of +/-(1%); +/-(3%); +/-%(5) change in gas prices (Data -> 2000- till date)' + '\\033[1m')\n",
        "print (\"The probability of Henry Hub gas price changes between 1%% and -1%% is %1.2f%% \" % \n",
        "       (100*daily_ret[(daily_ret > -0.01) & (daily_ret < 0.01)].shape[0] / daily_ret.shape[0]))\n",
        "print (\"The probability of Henry Hub gas price changes between 3%% and -3%% is %1.2f%% \" % \n",
        "       (100*daily_ret[(daily_ret > -0.03) & (daily_ret < 0.03)].shape[0] / daily_ret.shape[0]))\n",
        "print (\"The probability of Henry Hub gas price changes between 5%% and -5%% is %1.2f%% \" % \n",
        "       (100*daily_ret[(daily_ret > -0.05) & (daily_ret < 0.05)].shape[0] / daily_ret.shape[0]))\n",
        "print (\"The probability of Henry Hub gas price changes more than 5%% is %1.2f%%\" % \n",
        "       (100*daily_ret[daily_ret > 0.05].shape[0] / daily_ret.shape[0]))\n",
        "print (\"The probability of Henry Hub gas price changes less than -5%% is %1.2f%%\" % \n",
        "       (100*daily_ret[daily_ret < -0.05].shape[0] / daily_ret.shape[0]))\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mMinimum Gas Prices so far [2000- till date]\\033[0m')\n",
        "print(df['Open'].min(), df['Open'].idxmin());\n",
        "print('\\033[4mMaximum Gas Prices so far [2000- till date]\\033[0m')\n",
        "print(df['Open'].max(), df['Open'].idxmax());\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mMinimum Gas Prices so far- daily % return [2000- till date]\\033[0m')\n",
        "print(daily_ret.min(), daily_ret.idxmin()); \n",
        "print('\\033[4mMaximum Gas Prices so far- daily % return [2000- till date]\\033[0m')\n",
        "print(daily_ret.max(), daily_ret.idxmax());\n",
        "print('\\n')\n",
        "\n",
        "# Target Variable\n",
        "print('\\033[4mThis is a classification variable, average price will go either up or down the next day.\\033[0m')\n",
        "#This will be a classification variable, if the average price will go either up or down the next day.  \n",
        "#The target will be forecasting the difference between today’s price and tomorrow’s price (which is unkonwn).\n",
        "df['target'] = (df['sma1'].shift(-1) - df['sma1'])\n",
        "df = df.fillna(0)\n",
        "df['target'].tail()\n",
        "#print('\\n')\n",
        "\n",
        "def getBinary(val):\n",
        "    if val > 0:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "df['next_day_direction'] = df[\"target\"].apply(getBinary)\n",
        "print('\\033[4mThe target variables transformed for binary classification. A positive change in the value of prices classified as 1 and a non-positive change as 0.\\033[0m')\n",
        "\n",
        "#The target variables will be transformed for binary classification. \n",
        "#A positive change in the value of prices will be classified as 1 and a non-positive change as 0.\n",
        "#print(df.tail())\n",
        "#print('\\n')\n",
        "\n",
        "y = df['next_day_direction']\n",
        "x = df.drop(columns = ['next_day_direction', 'target', 'Last', 'High', 'Low', 'Volume'])\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "tscv = TimeSeriesSplit()\n",
        "#print(tscv)\n",
        "TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
        "for train_index, test_index in tscv.split(X):\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "eval_set = [(X_train, np.ravel(y_train)), (X_test, np.ravel(y_test))]\n",
        "xgb_clf1 = XGBClassifier(learning_rate= 0.01,\n",
        "                         max_depth= 5,\n",
        "                         min_child_weight= 3,\n",
        "                         n_estimators= 700,\n",
        "                         subsample =1,\n",
        "                         colsample_bytree = 0.5,\n",
        "                         gamma = 1)\n",
        "xgb_clf1.fit(X_train, y_train, \n",
        "             eval_metric = 'auc', eval_set = eval_set,\n",
        "             early_stopping_rounds = 5, verbose = 10)\n",
        "\n",
        "print('\\033[4mModel performance :: Quality of Classifier\\033[0m')\n",
        "plt.rcParams['figure.figsize'] = 15, 5 \n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "history = xgb_clf1.evals_result_\n",
        "x_axis = range(len(history['validation_0']['auc']))\n",
        "plt.plot(x_axis, history['validation_0']['auc'], label = 'Train')\n",
        "plt.plot(x_axis, history['validation_1']['auc'], label = 'Test')\n",
        "plt.legend(loc = 'best')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Xgboost AUC')\n",
        "plt.show()\n",
        "\n",
        "# we can then access the best number of tree and use it later for prediction\n",
        "#print('best iteration', grad_clf.best_ntree_limit)\n",
        "#print('\\n')\n",
        "\n",
        "# print the model's performance\n",
        "ntree_limit = xgb_clf1.best_ntree_limit\n",
        "print('\\033[4mNext day Open price direction\\033[0m')\n",
        "pred = xgb_clf1.predict(X_test, ntree_limit = ntree_limit)\n",
        "pred = pd.DataFrame(pred)\n",
        "print(pred.tail(15))\n",
        "print('\\n')\n",
        "\n",
        "pred_prob = xgb_clf1.predict_proba(X_test, ntree_limit = ntree_limit)[:, 1]\n",
        "print('Area under ROC curve:', roc_auc_score(y_test, pred_prob)*100)\n",
        "print('\\n')\n",
        "\n",
        "# Feature importance\n",
        "fig = plt.figure(figsize=(20,6))\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.bar([i for i in range(len(xgb_clf1.feature_importances_))], xgb_clf1.feature_importances_.tolist(), \n",
        "        tick_label=x.columns, color=\"chocolate\")\n",
        "plt.title('Feature importance plot')\n",
        "plt.show()\n",
        "print('\\n')\n",
        "\n",
        "#print('\\033[4mProbability of prediction results\\033[0m')\n",
        "#pred_prob = pd.DataFrame(pred_prob)\n",
        "#print(pred_prob.tail(15))\n",
        "#print('\\n')\n",
        "\n",
        "\n",
        "# For each day where open price > 0, we set the signal 1 means sell  and < 0, we set signal 0, to buy \n",
        "print('\\033[4mFor each day where open price = 0 = red arrow head  and 1 = green arrow head \\033[0m')\n",
        "buys = df.loc[df['next_day_direction'] == 0]\n",
        "sells = df.loc[df['next_day_direction'] == 1]\n",
        "\n",
        "# Plot \n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.plot(df.index, df['Last'], color = 'b', lw=2., label='Last')\n",
        "# Plot the buy and sell signals on the same plot\n",
        "plt.plot(buys.index, df.loc[buys.index]['Last'], '^', markersize=10, color='r')\n",
        "plt.plot(sells.index, df.loc[sells.index]['Last'], 'v', markersize=10, color='g')\n",
        "plt.ylabel('Price')\n",
        "plt.xlabel('Date')\n",
        "plt.title('Buy (Red) and sell (Green) signals')\n",
        "plt.legend(loc=0)\n",
        "# Display everything\n",
        "plt.show()\n",
        "print('\\n')\n",
        "\n",
        "print(df.loc[buys.index]['Last'])\n",
        "\n",
        "print('\\033[4mAverage Sharpe Ratio (risk-adjusted return):: Daily\\033[0m')\n",
        "sharpe_ratio = daily_ret.mean() / daily_ret.std()\n",
        "print('Sharpe Ratio:', sharpe_ratio)\n",
        "print('\\033[4mSharpe Ratio:: Annual\\033[0m')\n",
        "an_sharpe_ratio = (252**0.5) * sharpe_ratio # annualised sharpe ratio\n",
        "print('Annualized Sharpe Ratio (risk-adjusted return):', an_sharpe_ratio)\n",
        "print('\\n')\n",
        "\n",
        "## Moving averages crossover [2 months and 1 year look back]\n",
        "merge14 = merge12.apply(copy.deepcopy)\n",
        "\n",
        "# feature engineering\n",
        "merge14['day_of_week'] = merge14.index.dayofweek\n",
        "merge14['day_of_month'] = merge14.index.day\n",
        "merge14['quarter'] = merge14.index.quarter\n",
        "merge14['month'] = merge14.index.month\n",
        "merge14['year'] = merge14.index.year\n",
        "\n",
        "# Technical indicators\n",
        "merge14['daily_ret'] = merge14['Last'].pct_change()\n",
        "#merge14['volatility'] = merge14['daily_ret'].rolling(252).std()*(252**0.5)\n",
        "merge14[\"Last_1\"] = merge14['Last'].shift(1)\n",
        "merge14[\"Last_incr\"] = merge14['Last'] - merge14['Last'].shift(1)\n",
        "merge14['price_diff'] = merge14['Last'] - merge14['Open']\n",
        "merge14[\"vol_increment\"] = merge14['Volume'].diff()\n",
        "merge14[\"vol_rel_increment\"] = merge14['Volume'].diff() / merge14['Volume']\n",
        "\n",
        "sma1 = 42\n",
        "sma2 = 252\n",
        "merge14[\"sma1\"] = merge14['Last'].rolling(sma1).mean().fillna(0)\n",
        "merge14[\"sma2\"] = merge14['Last'].rolling(sma2).mean().fillna(0)\n",
        "merge14['ema42'] = merge14['Last'].ewm(span=42).mean().fillna(0)\n",
        "merge14['ema252'] = merge14['Last'].ewm(span=252).mean().fillna(0)\n",
        "\n",
        "merge14['ema_12'] = merge14['Last'].ewm(span=10).mean().fillna(0)\n",
        "merge14['ema_26'] = merge14['Last'].ewm(span=26).mean().fillna(0)\n",
        "merge14['ROC'] = ((merge14['Last'] - merge14['Last'].shift(5)) / (merge14['Last'].shift(5)))*100\n",
        "\n",
        "delta = merge14['Last'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge14['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge14['macd'] = merge14['ema_12'] - merge14['ema_26']\n",
        "\n",
        "\n",
        "df1 = merge14.apply(copy.deepcopy)\n",
        "df1.fillna(-99999, inplace=True)\n",
        "\n",
        "# moving average crossover :: sma 252 is > sma 42\n",
        "df1['target'] = np.where(df1['sma2'] > df1['sma1'], 1,0)\n",
        "df1 = df1.fillna(0)\n",
        "df1['target'].tail()\n",
        "#print('\\n')\n",
        "\n",
        "def getBinary(val):\n",
        "    if val>0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "df1['next_day_direction'] = df1[\"target\"].apply(getBinary)\n",
        "\n",
        "y1 = df1['next_day_direction']\n",
        "x1 = df1.drop(columns = ['next_day_direction', 'target', 'Last', 'High', 'Low', 'Volume',\n",
        "                         'day_of_week', 'day_of_month', 'daily_ret', 'Last_incr', 'price_diff',\n",
        "                         'vol_increment', 'vol_rel_increment'])\n",
        "\n",
        "X1 = np.array(x1)\n",
        "y1 = np.array(y1)\n",
        "tscv = TimeSeriesSplit()\n",
        "#print(tscv)\n",
        "TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
        "for train_index, test_index in tscv.split(X):\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X1_train, X1_test = X1[train_index], X1[test_index]\n",
        "  y1_train, y1_test = y1[train_index], y1[test_index]\n",
        "\n",
        "eval_set = [(X1_train, np.ravel(y1_train)), (X1_test, np.ravel(y1_test))]\n",
        "\n",
        "xgb_clf2 = XGBClassifier(learning_rate= 0.01,\n",
        "                         max_depth= 5,\n",
        "                         min_child_weight= 3,\n",
        "                         n_estimators= 700,\n",
        "                         subsample =1,\n",
        "                         colsample_bytree = 0.5,\n",
        "                         gamma = 1)\n",
        "xgb_clf2.fit(X1_train, y1_train, \n",
        "            eval_metric = 'auc', eval_set = eval_set,\n",
        "            early_stopping_rounds = 5, verbose = 10)\n",
        "\n",
        "print('\\033[4mModel performance :: Quality of Classifier\\033[0m')\n",
        "plt.rcParams['figure.figsize'] = 15, 5 \n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "history = xgb_clf2.evals_result_\n",
        "x_axis = range(len(history['validation_0']['auc']))\n",
        "plt.plot(x_axis, history['validation_0']['auc'], label = 'Train')\n",
        "plt.plot(x_axis, history['validation_1']['auc'], label = 'Test')\n",
        "plt.legend(loc = 'best')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Xgboost AUC')\n",
        "plt.show()\n",
        "\n",
        "# we can then access the best number of tree and use it later for prediction\n",
        "#print('best iteration', grad_clf.best_ntree_limit)\n",
        "#print('\\n')\n",
        "\n",
        "# Feature importance\n",
        "fig = plt.figure(figsize=(20,6))\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.bar([i for i in range(len(xgb_clf2.feature_importances_))], xgb_clf2.feature_importances_.tolist(), \n",
        "        tick_label=x1.columns)\n",
        "plt.title('Feature importance plot')\n",
        "plt.show()\n",
        "print('\\n')\n",
        "\n",
        "#print('\\033[4mProbability of prediction results\\033[0m')\n",
        "#pred_prob = pd.DataFrame(pred_prob)\n",
        "#print(pred_prob.tail(15))\n",
        "#print('\\n')\n",
        "\n",
        "# print the model's performance\n",
        "ntree_limit = xgb_clf2.best_ntree_limit\n",
        "prob = xgb_clf2.predict_proba(X1_test, ntree_limit = ntree_limit)[:, 1]\n",
        "print('Area under ROC curve:', roc_auc_score(y1_test, prob)*100)\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mNext day Open price direction\\033[0m')\n",
        "predict_prob2 = xgb_clf2.predict(X1_test, ntree_limit = ntree_limit)\n",
        "predict_prob2 = pd.DataFrame(predict_prob2)\n",
        "print(predict_prob2.tail(15))\n",
        "print('\\n')\n",
        "\n",
        "#print('\\033[4mSimple Moving Average cross-over:: 1 is when short (42) crosses long, 0 is when long crosses short\\033[0m')\n",
        "print('\\033[4mBuy-> Shorter MA crosses above the longer-term MA (Golden cross)\\033[0m')\n",
        "print('\\033[4mSell-> Shorter MA crosses below the longer-term MA(Dead cross)\\033[0m')\n",
        "#print('\\033[4mLong moving average (252) down, the signal 1 [Buy] and when long (252) is up, the set signal 0 [Sell]\\033[0m')\n",
        "buys = df1.loc[df1['next_day_direction'] == 0]\n",
        "sells = df1.loc[df1['next_day_direction'] == 1]\n",
        "\n",
        "print(\"buys data to checking\")\n",
        "print(df1.loc[buys.index]['Last'])\n",
        "\n",
        "# Plot \n",
        "fig = plt.figure(figsize=(20,6))\n",
        "plt.plot(merge14['Last'], color='gray', label='Last')\n",
        "# Plot the buy and sell signals on the same plot\n",
        "#  When the shorter-term MA crosses above the longer-term MA (Golden cross), it's a buy signal, it indicates that the trend is shifting up. \n",
        "#  When the shorter-term MA crosses below the longer-term MA, it's a sell signal (dead/death cross), it indicates that the trend is shifting down. \n",
        "plt.plot(merge14['sma1'].dropna(), color='r', label = 'sma42')\n",
        "plt.plot(merge14['sma2'].dropna(), color='g', label = 'sma252')\n",
        "plt.ylabel('Price')\n",
        "plt.xlabel('Date')\n",
        "plt.suptitle('SMA crossover')\n",
        "plt.title('Buy -> Shorter MA crosses below longer MA (Deadcross); Sell -> Shorter MA crosses above longer MA (Golden cross)')\n",
        "plt.legend(loc=0)\n",
        "# Display everything\n",
        "plt.show()\n",
        "print('\\n')\n",
        "\n",
        "## 5 days look-ahead Open price\n",
        "\n",
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "NG.reset_index(level=0, inplace=True)\n",
        "print(NG.tail())\n",
        "print('\\n')\n",
        "\n",
        "# Technical indicators\n",
        "merge16 = merge12.apply(copy.deepcopy)\n",
        "\n",
        "merge16['pct_change'] = merge16['Open'].pct_change()\n",
        "#merge16['volatility'] = merge16['pct_change'].rolling(252).std()*(252**0.5)\n",
        "#merge16[\"vol_increment\"] = merge16['Volume'].diff()\n",
        "merge16[\"vol_rel_increment\"] = merge16['Volume'].diff() / merge16['Volume']\n",
        "merge16['std_5'] = merge16['pct_change'].rolling(5).std()\n",
        "merge16['ret_5'] = merge16['pct_change'].rolling(5).mean()\n",
        "merge16['sma42'] = merge16['Open'].rolling(42).mean()\n",
        "#merge16[\"sma42_increment\"] = merge16['sma42'].diff()  \n",
        "merge16['sma252'] = merge16['Open'].rolling(252).mean()\n",
        "#merge16[\"sma252_increment\"] = merge16['sma252'].diff()\n",
        "merge16['ema_12'] = merge16['Open'].ewm(span=10).mean()\n",
        "merge16['ema_26'] = merge16['Open'].ewm(span=26).mean()\n",
        "merge16['ROC'] = ((merge16['Open'] - merge16['Open'].shift(5)) / (merge16['Open'].shift(5)))*100\n",
        "\n",
        "delta = merge16['Open'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge16['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge16['macd'] = merge16['ema_12'] - merge16['ema_26']\n",
        "\n",
        "df4 = merge16.apply(copy.deepcopy)\n",
        "df4.fillna(-99999, inplace=True)\n",
        "forecast_out = int(5) # predicting 5 days into future\n",
        "df4['Prediction'] = df4[['Open']].shift(-forecast_out) #  label column with data shifted 21 units up\n",
        "\n",
        "X3 = df4.drop(columns = ['Prediction', 'Last', 'High', 'Low', 'Volume'])\n",
        "X3 = np.array(X3)\n",
        "X3_forecast = X3[-forecast_out:] # set X_forecast equal to last 5, we do not have y values for X_forecast\n",
        "X3 = X3[:-forecast_out] # remove last 7 from X\n",
        "#df1.dropna(inplace=True)\n",
        "y3 = np.array(df4['Prediction'])\n",
        "y3 = y3[:-forecast_out]\n",
        "df4.dropna(inplace=True)\n",
        "\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = 0.2, random_state=42)\n",
        "\n",
        "reg2 = xgb.XGBRegressor(objective ='reg:squarederror', n_jobs=-1)\n",
        "\n",
        "# Create the model on train dataset\n",
        "reg2.fit(X3_train, y3_train)\n",
        "\n",
        "print('\\033[4mBacktesting\\033[0m')\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kf_cv_scores = cross_val_score(reg2, X3_train, y3_train, cv=kfold )\n",
        "print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n",
        "print('\\n')\n",
        "\n",
        "confidence = reg2.score(X3_test, y3_test)\n",
        "print(\"confidence: \", confidence)\n",
        "print('\\n')\n",
        "forecast_pred_2 = reg2.predict(X3_forecast)\n",
        "print('\\033[4mExpected Open price for next 5 days\\033[0m')\n",
        "print(forecast_pred_2)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "## 5 days look-ahead Close price\n",
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "NG.reset_index(level=0, inplace=True)\n",
        "print(NG.tail())\n",
        "print('\\n')\n",
        "\n",
        "# Technical indicators\n",
        "merge15 = merge12.apply(copy.deepcopy)\n",
        "\n",
        "merge15['pct_change'] = merge15['Last'].pct_change()\n",
        "merge15['std_5'] = merge15['pct_change'].rolling(5).std()\n",
        "merge15['ret_5'] = merge15['pct_change'].rolling(5).mean()\n",
        "#merge15['volatility'] = merge15['pct_change'].rolling(252).std()*(252**0.5)\n",
        "merge15['HL_pct'] = merge15['High'] - merge15['Low'] / merge15['Low']\n",
        "merge15['price_diff'] = merge15['Last'] - merge15['Open']\n",
        "#merge15[\"vol_increment\"] = merge15['Volume'].diff()\n",
        "merge15[\"vol_rel_increment\"] = merge15['Volume'].diff() / merge15['Volume']\n",
        "merge15['sma42'] = merge15['Last'].rolling(42).mean()\n",
        "#merge15[\"sma42_increment\"] = merge15['sma42'].diff() \n",
        "#merge15['sma252'] = merge15['Last'].rolling(252).mean()\n",
        "#merge15[\"sma252_increment\"] = merge15['sma252'].diff()\n",
        "merge15['ema_12'] = merge15['Last'].ewm(span=10).mean()\n",
        "merge15['ema_26'] = merge15['Last'].ewm(span=26).mean()\n",
        "merge15['ROC'] = ((merge15['Last'] - merge15['Last'].shift(5)) / (merge15['Last'].shift(5)))*100\n",
        "\n",
        "delta = merge14['Last'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge15['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge15['macd'] = merge15['ema_12'] - merge15['ema_26']\n",
        "\n",
        "df3 = merge15.apply(copy.deepcopy)\n",
        "df3.fillna(-99999, inplace=True)\n",
        "forecast_out = int(5) # predicting 5 days into future\n",
        "df3['Prediction'] = df3[['Last']].shift(-forecast_out) #  label column with data shifted 21 units up\n",
        "\n",
        "X2 = df3.drop(columns = ['Prediction', 'Open', 'High', 'Low', 'Volume'])\n",
        "X2 = np.array(X2)\n",
        "X2_forecast = X2[-forecast_out:] # set X1_forecast equal to last 5, we do not have y values for X_forecast\n",
        "X2 = X2[:-forecast_out] # remove last 5 from X1\n",
        "y2 = np.array(df3['Prediction'])\n",
        "y2 = y2[:-forecast_out]\n",
        "df1.dropna(inplace=True)\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state=42)\n",
        "\n",
        "reg1 = xgb.XGBRegressor(objective ='reg:squarederror', n_jobs=-1)\n",
        "\n",
        "# Create the model on train dataset\n",
        "reg1.fit(X2_train, y2_train)\n",
        "\n",
        "print('\\033[4mBacktesting\\033[0m')\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kf_cv_scores = cross_val_score(reg1, X2_train, y2_train, cv=kfold )\n",
        "print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n",
        "print('\\n')\n",
        "\n",
        "confidence = reg1.score(X2_test, y2_test)\n",
        "print(\"confidence: \", confidence)\n",
        "print('\\n')\n",
        "forecast_pred_1 = reg1.predict(X2_forecast)\n",
        "print('\\033[4mExpected Close price for next 5 days\\033[0m')\n",
        "print(forecast_pred_1)\n",
        "print('\\n')\n",
        "\n",
        "# plotting the data\n",
        "#df1['Forecast'] = np.nan\n",
        "\n",
        "#last_date = df1.iloc[-1].name\n",
        "#last_unix = last_date.timestamp()\n",
        "#one_day = 86400\n",
        "#next_unix = last_unix + one_day\n",
        "\n",
        "#for i in forecast_pred_1:\n",
        "  #next_date = datetime.datetime.fromtimestamp(next_unix)\n",
        "  #next_unix += one_day\n",
        "  #df1.loc[next_date] = [np.nan for _ in range(len(df1.columns)-1)] + [i]\n",
        "\n",
        "#fig = plt.figure(figsize=(15,6))\n",
        "#df1['Last'].plot()\n",
        "#df1['Forecast'].plot()\n",
        "#plt.legend(loc='best')\n",
        "#plt.xlabel('Date')\n",
        "#plt.ylabel('Price')\n",
        "#plt.title('Historical & 30 days look-ahead line: Close price')\n",
        "#plt.show()\n",
        "\n",
        "#forecast_pred_1 = pd.DataFrame(forecast_pred_1)\n",
        "#forecast_pred_1.rename(columns={0: \"Pred Close price\"}, inplace=True)\n",
        "#forecast_pred_1\n",
        "\n",
        "## Week, month, year historical analysis\n",
        "\n",
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "print('\\033[4mPrint 1st & last 5 rows\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "NG.reset_index(level=0, inplace=True)\n",
        "print(NG)\n",
        "print('\\n')\n",
        "\n",
        "# feature engineering\n",
        "#Adding Weeks, Months and Year Columns\n",
        "print('\\033[4mFeature engineering- Adding Weeks, Months and Year Columns\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "NG['week_no'] = NG['Date'].dt.week\n",
        "NG['month_no'] = NG['Date'].dt.month\n",
        "NG['year'] = NG['Date'].dt.year\n",
        "NG['DayofWeek'] = NG['Date'].dt.dayofweek\n",
        "print(NG.tail())\n",
        "print('\\n')\n",
        "\n",
        "fig = plt.figure(figsize=(15,6))\n",
        "plt.plot(NG[NG.year >= 2019].groupby('week_no')['Open'].mean().head(52))\n",
        "plt.title('Average weekly Henry Hub Futures (contract#1) price in a year')\n",
        "plt.show()\n",
        "print ('\\033[4mThe plot reveals that, the average weekly Henry Hub Futures (Contract#1) price in a year illustrates mainly downward curve.\\033[0m')\n",
        "print('\\n')\n",
        "\n",
        "# Average Weekly Gas Prices Pivot Table\n",
        "print('\\033[4mAverage Weekly Gas Prices Pivot Table\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "pivot_ng = NG.pivot_table(values = 'Open', columns = ['year'], aggfunc= np.mean,index = ['week_no'])\n",
        "print(pivot_ng.tail()) # last 5 rows\n",
        "print('\\n')\n",
        "\n",
        "# Average Monthly Gas Prices & Percent Changes\n",
        "print('\\033[4mAverage Monthly Gas Prices\\033[0m')\n",
        "monthly = NG.pivot_table(values= 'Open', columns = ['year'],aggfunc = np.mean, index = ['month_no'])\n",
        "print(monthly) \n",
        "print('\\n')\n",
        "\n",
        "monthly.loc[0] = monthly.loc[12,:].shift(1)\n",
        "monthly.fillna(method='ffill', inplace=True) # filling nan values with previous data\n",
        "monthly = monthly.sort_index()\n",
        "print(monthly)\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mAverage monthly gas prices % change\\033[0m')\n",
        "monthly_change = monthly.pct_change()\n",
        "# dropping the 1st row\n",
        "monthly_change = monthly_change.drop(monthly_change.index[0])\n",
        "print(monthly_change)\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mProbabilities of Monthly Gas Price Decline vs Raise\\033[0m')\n",
        "print('\\033[4mProbability of gas price decline starts to increase by the summer months. It seems that fluctuations in gas price in the initial months of the year is rather high.\\033[0m')\n",
        "monthly_change['raise'] = monthly_change[(monthly_change.iloc\n",
        "                                                              [:,:]>0)].count(axis=1)/(2020-2000)\n",
        "monthly_change['decline'] = monthly_change[(monthly_change.iloc\n",
        "                                                              [:,:]<0)].count(axis=1)/(2020-2000)   \n",
        "print(monthly_change[['raise', 'decline']])  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsqY8cNfX3q-",
        "colab_type": "text"
      },
      "source": [
        "### **Statistical summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrKbzgTU9L6l",
        "colab_type": "code",
        "outputId": "39b250e7-8e88-4a56-ba8a-7cd501b17fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install ffn\n",
        "import ffn\n",
        "print('\\n')\n",
        "\n",
        "#num_days = 1000\n",
        "#df = (np.random.randn(num_days) + np.random.uniform(low=0.0, high=0.2, size=num_days))\n",
        "#index = pd.date_range('01/01/2010',periods=num_days, freq='D')\n",
        "#data = pd.DataFrame(data,index=index,columns=['Returns'])\n",
        "#df['Last'] = df.cumsum() + 100\n",
        "#df.iloc[0] = 100\n",
        "\n",
        "perf = df['Last'].calc_stats()\n",
        "\n",
        "perf.display()\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mMonthly returns\\033[0m')\n",
        "print(perf.display_monthly_returns())\n",
        "print('\\n')\n",
        "\n",
        "#ffn.to_drawdown_series(df['Last']).plot(figsize=(15,7),grid=True)\n",
        "#print('\\n')\n",
        "\n",
        "print('\\033[4mStatistics\\033[0m')\n",
        "print(perf.stats)\n",
        "print('\\n')\n",
        "\n",
        "print('Yearly Sharpe Ration:', perf.stats['yearly_sharpe'])\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mDisplaying look-back returns\\033[0m')\n",
        "print(perf.display_lookback_returns())\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ffn in /usr/local/lib/python3.6/dist-packages (0.3.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from ffn) (0.25.3)\n",
            "Requirement already satisfied: scipy>=0.15 in /usr/local/lib/python3.6/dist-packages (from ffn) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.6/dist-packages (from ffn) (0.8.6)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.6/dist-packages (from ffn) (0.7.4)\n",
            "Requirement already satisfied: scikit-learn>=0.15 in /usr/local/lib/python3.6/dist-packages (from ffn) (0.22.1)\n",
            "Requirement already satisfied: matplotlib>=1 in /usr/local/lib/python3.6/dist-packages (from ffn) (3.1.3)\n",
            "Requirement already satisfied: numpy>=1.5 in /usr/local/lib/python3.6/dist-packages (from ffn) (1.17.5)\n",
            "Requirement already satisfied: future>=0.15 in /usr/local/lib/python3.6/dist-packages (from ffn) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4 in /usr/local/lib/python3.6/dist-packages (from ffn) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->ffn) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->ffn) (2018.9)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->ffn) (2.21.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->ffn) (4.2.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->ffn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.15->ffn) (0.14.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1->ffn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1->ffn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1->ffn) (2.4.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.19->ffn) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader>=0.2->ffn) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader>=0.2->ffn) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader>=0.2->ffn) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader>=0.2->ffn) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1->ffn) (45.2.0)\n",
            "\n",
            "\n",
            "Stats for Last from 2000-01-04 00:00:00 - 2020-03-06 00:00:00\n",
            "Annual risk-free rate considered: 0.00%\n",
            "Summary:\n",
            "Total Return      Sharpe  CAGR    Max Drawdown\n",
            "--------------  --------  ------  --------------\n",
            "-20.45%             0.25  -1.13%  -89.39%\n",
            "\n",
            "Annualized Returns:\n",
            "mtd    3m       6m       ytd      1y       3y       5y      10y     incep.\n",
            "-----  -------  -------  -------  -------  -------  ------  ------  --------\n",
            "1.76%  -26.31%  -30.45%  -20.71%  -39.09%  -15.55%  -9.42%  -9.17%  -1.13%\n",
            "\n",
            "Periodic:\n",
            "        daily    monthly    yearly\n",
            "------  -------  ---------  --------\n",
            "sharpe  0.25     0.22       0.01\n",
            "mean    14.15%   11.62%     0.29%\n",
            "vol     55.98%   53.47%     41.82%\n",
            "skew    1.53     0.70       0.65\n",
            "kurt    22.68    2.00       0.11\n",
            "best    58.02%   62.61%     86.34%\n",
            "worst   -32.80%  -41.62%    -73.71%\n",
            "\n",
            "Drawdowns:\n",
            "max      avg       # days\n",
            "-------  ------  --------\n",
            "-89.39%  -9.88%    186.15\n",
            "\n",
            "Misc:\n",
            "---------------  -------\n",
            "avg. up month    11.77%\n",
            "avg. down month  -10.39%\n",
            "up year %        40.00%\n",
            "12m up %         53.88%\n",
            "---------------  -------\n",
            "\n",
            "\n",
            "\u001b[4mMonthly returns\u001b[0m\n",
            "  Year     Jan     Feb     Mar     Apr     May     Jun     Jul     Aug    Sep     Oct     Nov     Dec     YTD\n",
            "------  ------  ------  ------  ------  ------  ------  ------  ------  -----  ------  ------  ------  ------\n",
            "  2000   22.33    3.72    6.66    6.66   38.68    2.75  -15.68   26.71   8.45  -13.42   46.75   48.35  349.22\n",
            "  2001  -41.62   -8.25   -4.03   -6.57  -16.63  -20.9     6.46  -27.79  -5.71   46.66  -17.93   -4.85  -73.71\n",
            "  2002  -16.81   10.24   39.29   15.6   -15.23    0.87   -8.97   11.58  25.55    0.43    1.06   14.02   86.34\n",
            "  2003   17.04   44.53  -37.54    6.42   16.08  -13.44  -12.81    0.28   2.09    1.3     0.65   25.66   29.23\n",
            "  2004  -12.8     0.35    9.55   -1.2     9.89   -4.46   -0.7   -16.98  33.92   28.4   -12.66  -19.3    -0.65\n",
            "  2005    2.8     6.47   13.71  -13.96   -3.13    9.44   12.95   45.49  21.35  -12.33    3.13  -10.82   82.55\n",
            "  2006  -17.01  -27.93    7.39   -9.08   -2.61   -4.39   34.52  -26.34  -7.08   34.06   17.39  -35.1   -48.86\n",
            "  2007   18.99    6.38    6.39    1.09    1.68  -14.75   -8     -12.25  25.64   21.25  -12.34    2.48   30.37\n",
            "  2008    7.9    16       7.85    7.35    7.93   14.1   -31.71  -12.9   -6.36   -8.81   -4.02  -13.64  -24.87\n",
            "  2009  -21.43   -4.96  -10.05  -10.67   13.7     0      -4.75  -18.51  62.61    4.21   -3.9    14.93   -0.89\n",
            "  2010   -7.91   -6.2   -19.61    1.32   10.74    6.33    6.65  -22.49   1.47    4.29    3.52    5.38  -20.94\n",
            "  2011    0.34   -8.67    8.72    7.04   -0.68   -6.26   -5.24   -2.2   -9.57    7.31   -9.76  -15.8   -32.15\n",
            "  2012  -16.26    4.51  -18.73    7.48    6      16.6    13.63  -12.78  18.61   11.2    -3.55   -5.9    12.11\n",
            "  2013   -0.36    4.4    15.43    7.93   -8.27  -10.52   -3.34    3.92  -0.59    0.59   10.42    6.98   26.23\n",
            "  2014   16.86   -6.76   -5.16   10.16   -5.67   -1.78  -14.03    6      1.38   -6.02    5.22  -29.1   -31.7\n",
            "  2015   -6.85    1.6    -3.44    4.2    -3.96    7.19   -4.17   -0.99  -6.18   -8.57   -3.25    5.38  -18.66\n",
            "  2016   -1.91  -26.03   15.13    9.07    6.91   28.18   -2.66    0.84   0.8     2.65   12.28   11.89   59.32\n",
            "  2017  -16.32  -11.65   15.32    2.41   -5.78   -1.27   -7.6     7.83  -0.4    -3.74    4.06   -2.25  -21.07\n",
            "  2018    0.1    -9.77    2.32    1.24    6.8    -1.15   -4.73    5.03   2.74    9.4    41.33  -36.21    0.17\n",
            "  2019   -4.56   -0.64   -4.77   -3.82   -4.16   -6.01   -3.32    1.83   2.06   13.36  -12.32   -5.62  -26.25\n",
            "  2020  -15.57   -7.7     1.76    0       0       0       0       0      0       0       0       0     -20.71\n",
            "None\n",
            "\n",
            "\n",
            "\u001b[4mStatistics\u001b[0m\n",
            "start                    2000-01-04 00:00:00\n",
            "end                      2020-03-06 00:00:00\n",
            "rf                                         0\n",
            "total_return                       -0.204504\n",
            "cagr                              -0.0112791\n",
            "max_drawdown                       -0.893874\n",
            "calmar                            -0.0126182\n",
            "mtd                                0.0176367\n",
            "three_month                        -0.263091\n",
            "six_month                           -0.30454\n",
            "ytd                                -0.207055\n",
            "one_year                           -0.390922\n",
            "three_year                          -0.15546\n",
            "five_year                         -0.0941757\n",
            "ten_year                          -0.0916954\n",
            "incep                             -0.0112791\n",
            "daily_sharpe                        0.252738\n",
            "daily_sortino                       0.460173\n",
            "daily_mean                          0.141481\n",
            "daily_vol                           0.559792\n",
            "daily_skew                            1.5312\n",
            "daily_kurt                            22.683\n",
            "best_day                            0.580238\n",
            "worst_day                          -0.327982\n",
            "monthly_sharpe                      0.217336\n",
            "monthly_sortino                     0.420812\n",
            "monthly_mean                        0.116203\n",
            "monthly_vol                          0.53467\n",
            "monthly_skew                        0.695174\n",
            "monthly_kurt                         1.99834\n",
            "best_month                          0.626134\n",
            "worst_month                        -0.416164\n",
            "yearly_sharpe                     0.00701479\n",
            "yearly_sortino                     0.0145784\n",
            "yearly_mean                       0.00293341\n",
            "yearly_vol                          0.418175\n",
            "yearly_skew                         0.646905\n",
            "yearly_kurt                         0.107137\n",
            "best_year                           0.863424\n",
            "worst_year                         -0.737084\n",
            "avg_drawdown                      -0.0987751\n",
            "avg_drawdown_days                    186.154\n",
            "avg_up_month                        0.117742\n",
            "avg_down_month                     -0.103869\n",
            "win_year_perc                            0.4\n",
            "twelve_month_win_perc               0.538793\n",
            "dtype: object\n",
            "\n",
            "\n",
            "Yearly Sharpe Ration: 0.007014786011060988\n",
            "\n",
            "\n",
            "\u001b[4mDisplaying look-back returns\u001b[0m\n",
            "mtd        1.76%\n",
            "3m       -26.31%\n",
            "6m       -30.45%\n",
            "ytd      -20.71%\n",
            "1y       -39.09%\n",
            "3y       -15.55%\n",
            "5y        -9.42%\n",
            "10y       -9.17%\n",
            "incep     -1.13%\n",
            "Name: Last, dtype: object\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ffn/core.py:501: FutureWarning:\n",
            "\n",
            "\n",
            ".ix is deprecated. Please use\n",
            ".loc for label based indexing or\n",
            ".iloc for positional indexing\n",
            "\n",
            "See the documentation here:\n",
            "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohTqvIVPw0Hb",
        "colab_type": "text"
      },
      "source": [
        "# ***Extra for record***\n",
        "\n",
        "\n",
        "## **Hyperparameter tunning [kept silent]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVPWqSX44ngG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter optimization\n",
        "#xgb = xgb_clf1.fit(X_train, y_train, early_stopping_rounds=50,  eval_metric=\"logloss\", eval_set=[(X_test, y_test)])\n",
        "\n",
        "# scores\n",
        "#from  sklearn.metrics import log_loss\n",
        "#log_train = log_loss(y_train, xgb.predict_proba(X_train)[:,1])\n",
        "#log_valid = log_loss(y_test, xgb.predict_proba(X_test)[:,1])\n",
        "\n",
        "\n",
        "#print('\\n-----------------------')\n",
        "#print('  logloss train: %.5f'%log_train)\n",
        "#print('  logloss valid: %.5f'%log_valid)\n",
        "#print('-----------------------')\n",
        "\n",
        "#print('\\nModel parameters...')\n",
        "#print(xgb.get_params())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJR3zwQPLmQ1",
        "colab_type": "text"
      },
      "source": [
        "# **Moving averages crossover [2 months and 1 year look back]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqSFBYEwvL7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merge14 = merge12.apply(copy.deepcopy)\n",
        "\n",
        "# feature engineering\n",
        "merge14['day_of_week'] = merge14.index.dayofweek\n",
        "merge14['day_of_month'] = merge14.index.day\n",
        "merge14['quarter'] = merge14.index.quarter\n",
        "merge14['month'] = merge14.index.month\n",
        "merge14['year'] = merge14.index.year\n",
        "\n",
        "# Technical indicators\n",
        "merge14['daily_ret'] = merge14['Last'].pct_change()\n",
        "#merge14['volatility'] = merge14['daily_ret'].rolling(252).std()*(252**0.5)\n",
        "merge14[\"Last_1\"] = merge14['Last'].shift(1)\n",
        "merge14[\"Last_incr\"] = merge14['Last'] - merge14['Last'].shift(1)\n",
        "merge14['price_diff'] = merge14['Last'] - merge14['Open']\n",
        "merge14[\"vol_increment\"] = merge14['Volume'].diff()\n",
        "merge14[\"vol_rel_increment\"] = merge14['Volume'].diff() / merge14['Volume']\n",
        "\n",
        "sma1 = 42\n",
        "sma2 = 252\n",
        "merge14[\"sma1\"] = merge14['Last'].rolling(sma1).mean().fillna(0)\n",
        "merge14[\"sma2\"] = merge14['Last'].rolling(sma2).mean().fillna(0)\n",
        "merge14['ema42'] = merge14['Last'].ewm(span=42).mean().fillna(0)\n",
        "merge14['ema252'] = merge14['Last'].ewm(span=252).mean().fillna(0)\n",
        "\n",
        "merge14['ema_12'] = merge14['Last'].ewm(span=10).mean().fillna(0)\n",
        "merge14['ema_26'] = merge14['Last'].ewm(span=26).mean().fillna(0)\n",
        "merge14['ROC'] = ((merge14['Last'] - merge14['Last'].shift(5)) / (merge14['Last'].shift(5)))*100\n",
        "\n",
        "delta = merge14['Last'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge14['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge14['macd'] = merge14['ema_12'] - merge14['ema_26']\n",
        "\n",
        "\n",
        "df1 = merge14.apply(copy.deepcopy)\n",
        "df1.fillna(-99999, inplace=True)\n",
        "\n",
        "# moving average crossover :: sma 252 is > sma 42\n",
        "df1['target'] = np.where(df1['sma2'] > df1['sma1'], 1,0)\n",
        "df1 = df1.fillna(0)\n",
        "df1['target'].tail()\n",
        "#print('\\n')\n",
        "\n",
        "def getBinary(val):\n",
        "    if val>0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "df1['next_day_direction'] = df1[\"target\"].apply(getBinary)\n",
        "\n",
        "y1 = df1['next_day_direction']\n",
        "x1 = df1.drop(columns = ['next_day_direction', 'target', 'Last', 'High', 'Low', 'Volume',\n",
        "                         'day_of_week', 'day_of_month', 'daily_ret', 'Last_incr', 'price_diff',\n",
        "                         'vol_increment', 'vol_rel_increment'])\n",
        "\n",
        "X1 = np.array(x1)\n",
        "y1 = np.array(y1)\n",
        "tscv = TimeSeriesSplit()\n",
        "#print(tscv)\n",
        "TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
        "for train_index, test_index in tscv.split(X):\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X1_train, X1_test = X1[train_index], X1[test_index]\n",
        "  y1_train, y1_test = y1[train_index], y1[test_index]\n",
        "\n",
        "eval_set = [(X1_train, np.ravel(y1_train)), (X1_test, np.ravel(y1_test))]\n",
        "\n",
        "xgb_clf2 = XGBClassifier(learning_rate= 0.01,\n",
        "                         max_depth= 5,\n",
        "                         min_child_weight= 3,\n",
        "                         n_estimators= 700,\n",
        "                         subsample =1,\n",
        "                         colsample_bytree = 0.5,\n",
        "                         gamma = 1)\n",
        "xgb_clf2.fit(X1_train, y1_train, \n",
        "            eval_metric = 'auc', eval_set = eval_set,\n",
        "            early_stopping_rounds = 5, verbose = 10)\n",
        "\n",
        "print('\\033[4mModel performance :: Quality of Classifier\\033[0m')\n",
        "plt.rcParams['figure.figsize'] = 15, 5 \n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "history = xgb_clf2.evals_result_\n",
        "x_axis = range(len(history['validation_0']['auc']))\n",
        "plt.plot(x_axis, history['validation_0']['auc'], label = 'Train')\n",
        "plt.plot(x_axis, history['validation_1']['auc'], label = 'Test')\n",
        "plt.legend(loc = 'best')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Xgboost AUC')\n",
        "plt.show()\n",
        "\n",
        "# we can then access the best number of tree and use it later for prediction\n",
        "#print('best iteration', grad_clf.best_ntree_limit)\n",
        "#print('\\n')\n",
        "\n",
        "# Feature importance\n",
        "fig = plt.figure(figsize=(20,6))\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.bar([i for i in range(len(xgb_clf2.feature_importances_))], xgb_clf2.feature_importances_.tolist(), \n",
        "        tick_label=x1.columns)\n",
        "plt.title('Feature importance plot')\n",
        "plt.show()\n",
        "print('\\n')\n",
        "\n",
        "#print('\\033[4mProbability of prediction results\\033[0m')\n",
        "#pred_prob = pd.DataFrame(pred_prob)\n",
        "#print(pred_prob.tail(15))\n",
        "#print('\\n')\n",
        "\n",
        "# print the model's performance\n",
        "ntree_limit = xgb_clf2.best_ntree_limit\n",
        "prob = xgb_clf2.predict_proba(X1_test, ntree_limit = ntree_limit)[:, 1]\n",
        "print('Area under ROC curve:', roc_auc_score(y1_test, prob)*100)\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mNext day Open price direction\\033[0m')\n",
        "predict_prob2 = xgb_clf2.predict(X1_test, ntree_limit = ntree_limit)\n",
        "predict_prob2 = pd.DataFrame(predict_prob2)\n",
        "print(predict_prob2.tail(15))\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mSimple Moving Average cross-over:: 1 is when short (42) crosses long, 0 is when long crosses short\\033[0m')\n",
        "\n",
        "print('\\033[4mLong moving average (252) down, the signal 1 [Buy] and when long (252) is up, the set signal 0 [Sell]\\033[0m')\n",
        "buys = df1.loc[df1['next_day_direction'] == 0]\n",
        "sells = df1.loc[df1['next_day_direction'] == 1]\n",
        "\n",
        "# Plot \n",
        "fig = plt.figure(figsize=(20,6))\n",
        "plt.plot(merge14['Last'], color='gray', label='Last')\n",
        "# Plot the buy and sell signals on the same plot\n",
        "plt.plot(merge14['sma1'].dropna(), color='r', label = 'sma42')\n",
        "plt.plot(merge14['sma2'].dropna(), color='g', label = 'sma252')\n",
        "plt.ylabel('Price')\n",
        "plt.xlabel('Date')\n",
        "plt.title('SMA crossover')\n",
        "plt.legend(loc=0)\n",
        "# Display everything\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiJFqTx8P_bD",
        "colab_type": "text"
      },
      "source": [
        "# **Hyperparameter tunning [kept silent]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OssHm_pi8deU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter optimization\n",
        "#xgb = grad_clf.fit(train_x, train_y, early_stopping_rounds=50,  eval_metric=\"logloss\", eval_set=[(test_x, test_y)])\n",
        "\n",
        "# scores\n",
        "#from  sklearn.metrics import log_loss\n",
        "#log_train = log_loss(train_y, xgb.predict_proba(train_x)[:,1])\n",
        "#log_valid = log_loss(test_y, xgb.predict_proba(test_x)[:,1])\n",
        "\n",
        "\n",
        "#print('\\n-----------------------')\n",
        "#print('  logloss train: %.5f'%log_train)\n",
        "#print('  logloss valid: %.5f'%log_valid)\n",
        "#print('-----------------------')\n",
        "\n",
        "#print('\\nModel parameters...')\n",
        "#print(xgb.get_params())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32UYGO1j0To-",
        "colab_type": "text"
      },
      "source": [
        "#**5 days look-ahead Open price**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSHV4wbQxERv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "NG.reset_index(level=0, inplace=True)\n",
        "print(NG.tail())\n",
        "print('\\n')\n",
        "\n",
        "# Technical indicators\n",
        "merge16 = merge12.apply(copy.deepcopy)\n",
        "\n",
        "merge16['pct_change'] = merge16['Open'].pct_change()\n",
        "#merge16['volatility'] = merge16['pct_change'].rolling(252).std()*(252**0.5)\n",
        "#merge16[\"vol_increment\"] = merge16['Volume'].diff()\n",
        "merge16[\"vol_rel_increment\"] = merge16['Volume'].diff() / merge16['Volume']\n",
        "merge16['std_5'] = merge16['pct_change'].rolling(5).std()\n",
        "merge16['ret_5'] = merge16['pct_change'].rolling(5).mean()\n",
        "merge16['sma42'] = merge16['Open'].rolling(42).mean()\n",
        "#merge16[\"sma42_increment\"] = merge16['sma42'].diff()  \n",
        "merge16['sma252'] = merge16['Open'].rolling(252).mean()\n",
        "#merge16[\"sma252_increment\"] = merge16['sma252'].diff()\n",
        "merge16['ema_12'] = merge16['Open'].ewm(span=10).mean()\n",
        "merge16['ema_26'] = merge16['Open'].ewm(span=26).mean()\n",
        "merge16['ROC'] = ((merge16['Open'] - merge16['Open'].shift(5)) / (merge16['Open'].shift(5)))*100\n",
        "\n",
        "delta = merge16['Open'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge16['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge16['macd'] = merge16['ema_12'] - merge16['ema_26']\n",
        "\n",
        "df4 = merge16.apply(copy.deepcopy)\n",
        "df4.fillna(-99999, inplace=True)\n",
        "forecast_out = int(5) # predicting 5 days into future\n",
        "df4['Prediction'] = df4[['Open']].shift(-forecast_out) #  label column with data shifted 21 units up\n",
        "\n",
        "X3 = df4.drop(columns = ['Prediction', 'Last', 'High', 'Low', 'Volume'])\n",
        "X3 = np.array(X3)\n",
        "X3_forecast = X3[-forecast_out:] # set X_forecast equal to last 5, we do not have y values for X_forecast\n",
        "X3 = X3[:-forecast_out] # remove last 7 from X\n",
        "#df1.dropna(inplace=True)\n",
        "y3 = np.array(df4['Prediction'])\n",
        "y3 = y3[:-forecast_out]\n",
        "df4.dropna(inplace=True)\n",
        "\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = 0.2, random_state=42)\n",
        "\n",
        "reg2 = xgb.XGBRegressor(objective ='reg:squarederror', n_jobs=-1)\n",
        "\n",
        "# Create the model on train dataset\n",
        "reg2.fit(X3_train, y3_train)\n",
        "\n",
        "print('\\033[4mBacktesting\\033[0m')\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kf_cv_scores = cross_val_score(reg2, X3_train, y3_train, cv=kfold )\n",
        "print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n",
        "print('\\n')\n",
        "\n",
        "confidence = reg2.score(X3_test, y3_test)\n",
        "print(\"confidence: \", confidence)\n",
        "print('\\n')\n",
        "forecast_pred_2 = reg2.predict(X3_forecast)\n",
        "print('\\033[4mExpected Open price for next 5 days\\033[0m')\n",
        "print(forecast_pred_2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5edw2NvB0HeW",
        "colab_type": "text"
      },
      "source": [
        "# **5 days look-ahead Close price**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvjpQbH10kJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "NG.reset_index(level=0, inplace=True)\n",
        "print(NG.tail())\n",
        "print('\\n')\n",
        "\n",
        "# Technical indicators\n",
        "merge15 = merge12.apply(copy.deepcopy)\n",
        "\n",
        "merge15['pct_change'] = merge15['Last'].pct_change()\n",
        "merge15['std_5'] = merge15['pct_change'].rolling(5).std()\n",
        "merge15['ret_5'] = merge15['pct_change'].rolling(5).mean()\n",
        "#merge15['volatility'] = merge15['pct_change'].rolling(252).std()*(252**0.5)\n",
        "merge15['HL_pct'] = merge15['High'] - merge15['Low'] / merge15['Low']\n",
        "merge15['price_diff'] = merge15['Last'] - merge15['Open']\n",
        "#merge15[\"vol_increment\"] = merge15['Volume'].diff()\n",
        "merge15[\"vol_rel_increment\"] = merge15['Volume'].diff() / merge15['Volume']\n",
        "merge15['sma42'] = merge15['Last'].rolling(42).mean()\n",
        "#merge15[\"sma42_increment\"] = merge15['sma42'].diff() \n",
        "#merge15['sma252'] = merge15['Last'].rolling(252).mean()\n",
        "#merge15[\"sma252_increment\"] = merge15['sma252'].diff()\n",
        "merge15['ema_12'] = merge15['Last'].ewm(span=10).mean()\n",
        "merge15['ema_26'] = merge15['Last'].ewm(span=26).mean()\n",
        "merge15['ROC'] = ((merge15['Last'] - merge15['Last'].shift(5)) / (merge15['Last'].shift(5)))*100\n",
        "\n",
        "delta = merge14['Last'].diff()\n",
        "window = 14\n",
        "up_days = delta.copy()\n",
        "up_days[delta<=0]=0.0\n",
        "down_days = abs(delta.copy())\n",
        "down_days[delta>0]=0.0\n",
        "RS_up = up_days.rolling(window).mean()\n",
        "RS_down = down_days.rolling(window).mean()\n",
        "merge15['rsi'] = 100-100/(1+RS_up/RS_down)\n",
        "merge15['macd'] = merge15['ema_12'] - merge15['ema_26']\n",
        "\n",
        "df3 = merge15.apply(copy.deepcopy)\n",
        "df3.fillna(-99999, inplace=True)\n",
        "forecast_out = int(5) # predicting 5 days into future\n",
        "df3['Prediction'] = df3[['Last']].shift(-forecast_out) #  label column with data shifted 21 units up\n",
        "\n",
        "X2 = df3.drop(columns = ['Prediction', 'Open', 'High', 'Low', 'Volume'])\n",
        "X2 = np.array(X2)\n",
        "X2_forecast = X2[-forecast_out:] # set X1_forecast equal to last 5, we do not have y values for X_forecast\n",
        "X2 = X2[:-forecast_out] # remove last 5 from X1\n",
        "y2 = np.array(df3['Prediction'])\n",
        "y2 = y2[:-forecast_out]\n",
        "df1.dropna(inplace=True)\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state=42)\n",
        "\n",
        "reg1 = xgb.XGBRegressor(objective ='reg:squarederror', n_jobs=-1)\n",
        "\n",
        "# Create the model on train dataset\n",
        "reg1.fit(X2_train, y2_train)\n",
        "\n",
        "print('\\033[4mBacktesting\\033[0m')\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kf_cv_scores = cross_val_score(reg1, X2_train, y2_train, cv=kfold )\n",
        "print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n",
        "print('\\n')\n",
        "\n",
        "confidence = reg1.score(X2_test, y2_test)\n",
        "print(\"confidence: \", confidence)\n",
        "print('\\n')\n",
        "forecast_pred_1 = reg1.predict(X2_forecast)\n",
        "print('\\033[4mExpected Close price for next 5 days\\033[0m')\n",
        "print(forecast_pred_1)\n",
        "\n",
        "# plotting the data\n",
        "#df1['Forecast'] = np.nan\n",
        "\n",
        "#last_date = df1.iloc[-1].name\n",
        "#last_unix = last_date.timestamp()\n",
        "#one_day = 86400\n",
        "#next_unix = last_unix + one_day\n",
        "\n",
        "#for i in forecast_pred_1:\n",
        "  #next_date = datetime.datetime.fromtimestamp(next_unix)\n",
        "  #next_unix += one_day\n",
        "  #df1.loc[next_date] = [np.nan for _ in range(len(df1.columns)-1)] + [i]\n",
        "\n",
        "#fig = plt.figure(figsize=(15,6))\n",
        "#df1['Last'].plot()\n",
        "#df1['Forecast'].plot()\n",
        "#plt.legend(loc='best')\n",
        "#plt.xlabel('Date')\n",
        "#plt.ylabel('Price')\n",
        "#plt.title('Historical & 30 days look-ahead line: Close price')\n",
        "#plt.show()\n",
        "\n",
        "#forecast_pred_1 = pd.DataFrame(forecast_pred_1)\n",
        "#forecast_pred_1.rename(columns={0: \"Pred Close price\"}, inplace=True)\n",
        "#forecast_pred_1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAX8QwbVVhu1",
        "colab_type": "text"
      },
      "source": [
        "#**Week, month, year historical analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H2i_pJySJbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\033[4mNatural Gas continuous contract 1\\033[0m')\n",
        "print('\\033[4mPrint 1st & last 5 rows\\033[0m')\n",
        "NG = quandl.get(\"CHRIS/CME_NG1\", authtoken=\"LSQpgUzwJRoF667ZpzyL\") # natural gas continuous contract 1\n",
        "NG = NG.loc['2000-01-01':,['Open', 'High', 'Low', 'Last', 'Volume']]\n",
        "NG.reset_index(level=0, inplace=True)\n",
        "print(NG)\n",
        "print('\\n')\n",
        "\n",
        "# feature engineering\n",
        "#Adding Weeks, Months and Year Columns\n",
        "print('\\033[4mFeature engineering- Adding Weeks, Months and Year Columns\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "NG['week_no'] = NG['Date'].dt.week\n",
        "NG['month_no'] = NG['Date'].dt.month\n",
        "NG['year'] = NG['Date'].dt.year\n",
        "NG['DayofWeek'] = NG['Date'].dt.dayofweek\n",
        "print(NG.tail())\n",
        "print('\\n')\n",
        "\n",
        "fig = plt.figure(figsize=(15,6))\n",
        "plt.plot(NG[NG.year >= 2019].groupby('week_no')['Open'].mean().head(52))\n",
        "plt.title('Average weekly Henry Hub Futures (contract#1) price in a year')\n",
        "plt.show()\n",
        "print ('\\033[4mThe plot reveals that, the average weekly Henry Hub Futures (Contract#1) price in a year illustrates mainly downward curve.\\033[0m')\n",
        "print('\\n')\n",
        "\n",
        "# Average Weekly Gas Prices Pivot Table\n",
        "print('\\033[4mAverage Weekly Gas Prices Pivot Table\\033[0m')\n",
        "print('\\033[4mPrint last 5 rows\\033[0m')\n",
        "pivot_ng = NG.pivot_table(values = 'Open', columns = ['year'], aggfunc= np.mean,index = ['week_no'])\n",
        "print(pivot_ng.tail()) # last 5 rows\n",
        "print('\\n')\n",
        "\n",
        "# Average Monthly Gas Prices & Percent Changes\n",
        "print('\\033[4mAverage Monthly Gas Prices\\033[0m')\n",
        "monthly = NG.pivot_table(values= 'Open', columns = ['year'],aggfunc = np.mean, index = ['month_no'])\n",
        "print(monthly) \n",
        "print('\\n')\n",
        "\n",
        "monthly.loc[0] = monthly.loc[12,:].shift(1)\n",
        "monthly.fillna(method='ffill', inplace=True) # filling nan values with previous data\n",
        "monthly = monthly.sort_index()\n",
        "print(monthly)\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mAverage monthly gas prices % change\\033[0m')\n",
        "monthly_change = monthly.pct_change()\n",
        "# dropping the 1st row\n",
        "monthly_change = monthly_change.drop(monthly_change.index[0])\n",
        "print(monthly_change)\n",
        "print('\\n')\n",
        "\n",
        "print('\\033[4mProbabilities of Monthly Gas Price Decline vs Raise\\033[0m')\n",
        "print('\\033[4mProbability of gas price decline starts to increase by the summer months. It seems that fluctuations in gas price in the initial months of the year is rather high.\\033[0m')\n",
        "monthly_change['raise'] = monthly_change[(monthly_change.iloc\n",
        "                                                              [:,:]>0)].count(axis=1)/(2020-2000)\n",
        "monthly_change['decline'] = monthly_change[(monthly_change.iloc\n",
        "                                                              [:,:]<0)].count(axis=1)/(2020-2000)   \n",
        "print(monthly_change[['raise', 'decline']])  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}